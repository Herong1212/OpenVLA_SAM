"""
finetune.py

Simple script for parameter-efficient fine-tuning of OpenVLA models loaded through the HuggingFace AutoClasses, using
HuggingFace PEFT library for low-rank adaptation (LoRA).

Notes & Benchmarks:
    - Requires PEFT (`pip install peft==0.11.1`)
    - LoRA fine-tuning (see parameters below -- no quantization, LoRA rank = 32, target_modules = all-linear):
        + One 48 GB GPU can fit a Batch Size of 12
        + One 80 GB GPU can fit a Batch Size of 24

Run with:
    å‚æ•°è¯¦è§£ï¼š
        --standalone: å‘Šè¯‰ PyTorch æˆ‘æ˜¯å•æœºè¿è¡Œï¼Œä¸éœ€è¦å»è¿æ¥å…¶ä»–æœåŠ¡å™¨çš„ä¸»èŠ‚ç‚¹ï¼›
        --nnodes 1: åªæœ‰ 1 å°æœºå™¨ï¼ˆå°±æ˜¯ä½ è¿™ä¸€å°ï¼‰ï¼›
        --nproc-per-node $K: $K ä»£è¡¨ä½ è¦ç”¨å‡ å¼ å¡ï¼Œå°† nproc-per-node è®¾ç½®ä¸ºå¯ç”¨ GPU æ•°é‡
            å¦‚æœä½ æƒ³ 8 å¡å…¨å¼€ï¼šå°±æŠŠ $K æ¢æˆ 8ã€‚
            å¦‚æœä½ åªæƒ³ç”¨å‰ 2 å¼ å¡ï¼šå°±æŠŠ $K æ¢æˆ 2ã€‚
        
    - [Single Node Multi-GPU (= $K) ]: torchrun --standalone --nnodes 1 --nproc-per-node $K vla-scripts/finetune.py
    - [Override Config Values]: torchrun --standalone --nnodes 1 --nproc-per-node $K vla-scripts/finetune.py \
                                    --data_root_dir <PATH/TO/RLDS/DATASETS/DIRECTORY> \
                                    --dataset_name <DATASET_NAME> \
                                    --run_root_dir <PATH/TO/LOGS/DIR> \
                                    ...
"""

import os
from datetime import datetime
from collections import deque
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

# ä¸€ä¸ªåŸºäº dataclass çš„å‚æ•°è§£æåº“ï¼Œæ¯” argparse æ›´é«˜çº§ï¼Œæ”¯æŒå±‚çº§é…ç½®
import draccus

import torch
import torch.distributed as dist  # ç”¨äºå¤šå¡è®­ç»ƒæ—¶çš„è¿›ç¨‹é—´é€šä¿¡
import tqdm

from accelerate import PartialState  # HuggingFace Accelerate åº“ï¼Œç®€åŒ–å¤š GPU è®¾å¤‡ç®¡ç†

# PEFT (Parameter-Efficient Fine-Tuning) åº“æ ¸å¿ƒç»„ä»¶ï¼š
#   - LoraConfig: é…ç½® LoRA çš„å‚æ•°ï¼ˆå¦‚ç§© r, alphaï¼‰
#   - PeftModel: PEFT æ¨¡å‹çš„åŒ…è£…ç±»
#   - get_peft_model: å°†åŸºç¡€æ¨¡å‹åŒ…è£…æˆ PEFT æ¨¡å‹ï¼ˆå†»ç»“åŸå‚æ•°ï¼Œæ’å…¥ LoRA å±‚ï¼‰
#   - prepare_model_for_kbit_training: ä¸“ä¸ºé‡åŒ–è®­ç»ƒï¼ˆQLoRAï¼‰åšçš„é¢„å¤„ç†ï¼Œæ¯”å¦‚ç¨³å®š LayerNorm
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training

from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import AdamW
from torch.utils.data import DataLoader

# Transformers åº“æ ¸å¿ƒï¼š
#   - AutoModelForVision2Seq: è‡ªåŠ¨åŠ è½½è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰
#   - AutoProcessor: è‡ªåŠ¨åŠ è½½å¤„ç†å™¨ï¼ˆåŒ…å« Tokenizer å’Œ ImageProcessorï¼‰
#   - BitsAndBytesConfig: ç”¨äºé…ç½® 4-bit/8-bit é‡åŒ–çš„å‚æ•°
#   - Autoconfigï¼šåŠ è½½æ¨¡å‹çš„ â€œé…ç½®â€ (config.json)ï¼Œè€Œä¸åŠ è½½é‚£å‡ å GB çš„æƒé‡æ–‡ä»¶
#   - AutoImageProcessorï¼šåŠ è½½å›¾åƒçš„ â€œé¢„å¤„ç†å™¨â€ï¼Œå†³å®šå›¾ç‰‡åœ¨å–‚ç»™æ¨¡å‹å‰æ€ä¹ˆå¤„ç†
#   - CausalLMOutputWithPast: ä¸€ä¸ªæ•°æ®ç±» (Data Class)ï¼Œä¸“é—¨ç”¨æ¥å®šä¹‰æ¨¡å‹ â€œè¾“å‡ºç»“æœâ€çš„æ ¼å¼
from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig
from transformers import AutoConfig, AutoImageProcessor
from transformers.modeling_outputs import CausalLMOutputWithPast

# ç”¨äºå®éªŒè®°å½•å’Œå¯è§†åŒ–çš„å·¥å…·
import wandb

from prismatic.models.backbones.llm.prompting import PurePromptBuilder, VicunaV15ChatPromptBuilder
from prismatic.util.data_utils import PaddedCollatorForActionPrediction
from prismatic.vla.action_tokenizer import ActionTokenizer
from prismatic.vla.datasets import RLDSBatchTransform, RLDSDataset
from prismatic.vla.datasets.rlds.utils.data_utils import save_dataset_statistics

from prismatic.extern.hf.configuration_prismatic import OpenVLAConfig
from prismatic.extern.hf.modeling_prismatic import OpenVLAForActionPrediction
from prismatic.extern.hf.processing_prismatic import PrismaticImageProcessor, PrismaticProcessor

# Sane Defaults
# å¼ºåˆ¶å…³é—­ Hugging Face tokenizers åº“å†…éƒ¨çš„å¤šçº¿ç¨‹å¹¶è¡ŒåŠŸèƒ½
# åŸå› ï¼šHugging Face çš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰é»˜è®¤æ˜¯ç”¨ Rust å†™çš„ï¼Œä¸ºäº†å¿«ï¼Œå®ƒè‡ªå·±ä¼šåœ¨åå°å¼€å¤šçº¿ç¨‹ï¼›
# å†²çªï¼šä½†æ˜¯ï¼Œä½ çš„æ•°æ®åŠ è½½å™¨ DataLoaderï¼ˆåŸºäº PyTorch æˆ– RLDSï¼‰é€šå¸¸ä¹Ÿä¼šå¼€å¤šè¿›ç¨‹ (num_workers) æ¥è¯»å–æ•°æ®ï¼›
# åæœï¼šå½“â€œå¤šè¿›ç¨‹â€é‡Œé¢åµŒå¥—â€œå¤šçº¿ç¨‹â€æ—¶ï¼Œåœ¨ Linux ç³»ç»Ÿä¸‹ææ˜“å‘ç”Ÿ æ­»é”(Deadlock)ï¼Œå¯¼è‡´ç¨‹åºå¡æ­»ä¸åŠ¨ï¼Œæˆ–è€… CPU å ç”¨ç‡ 100 % å´ä¸å¹²æ´»ï¼›
# è§£å†³ï¼šå› æ­¤ï¼Œè¿™é‡Œå°† Hugging Face tokenizers åº“å†…éƒ¨å¤šçº¿ç¨‹å¹¶è¡ŒåŠŸèƒ½å…³é—­ï¼Œé˜²æ­¢è®­ç»ƒè¿‡ç¨‹ä¸­ç¨‹åºè«åå…¶å¦™å¡æ­»ã€‚
os.environ["TOKENIZERS_PARALLELISM"] = "false"


# # === Utilities ===
# # fmt: off
# def create_vision_transform(vla: nn.Module, input_size: int) -> Callable[[Image.Image], torch.Tensor]:
#     """Gets image transform for the vision encoder."""
#     data_cfg = timm.data.resolve_model_data_config(vla.vision_backbone)
#     data_cfg["input_size"] = (3, input_size, input_size)
#     return timm.data.create_transform(
#         input_size=data_cfg["input_size"],
#         interpolation=data_cfg["interpolation"],
#         mean=data_cfg["mean"],
#         std=data_cfg["std"],
#         crop_pct=1.0,           # Set to 1.0 to disable cropping
#         crop_mode="center",     # Default crop mode --> no-op when `crop_pct == 1.0`
#         is_training=False,      # Disable image_aug when loading transform; handled by RLDS dataloader
#     )
#
# # fmt: on


@dataclass
class FinetuneConfig:
    # fmt: off
    vla_path: str = "openvla/openvla-7b"                            # Path to OpenVLA model (on HuggingFace Hub)

    # Directory Paths
    data_root_dir: Path = Path("modified_libero_rlds")              # Path to Open-X dataset directoryï¼Œå³ï¼šæ•°æ®é›†è·¯å¾„ã€‚è¿™é‡Œå¿…é¡»å¡« RLDS æ ¼å¼æ•°æ®çš„æ ¹ç›®å½•ã€‚
    dataset_name: str = "libero_spatial_no_noops"                   # Name of fine-tuning dataset (e.g., `droid_wipe`)ï¼Œå³ï¼šæ•°æ®é›†åç§°ã€‚å¯¹åº” RLDS ç›®å½•ä¸‹çš„å­æ–‡ä»¶å¤¹åã€‚
    run_root_dir: Path = Path("runs")                               # Path to directory to store logs & checkpointsï¼Œå³ï¼šå®éªŒç»“æœå­˜æ”¾å¤„ã€‚æ—¥å¿—ã€è®­ç»ƒè¿‡ç¨‹ä¸­çš„ Checkpointã€æœ€ç»ˆæ¨¡å‹éƒ½ä¼šå­˜åœ¨è¿™ã€‚
    adapter_tmp_dir: Path = Path("adapter-tmp")                     # Temporary directory for LoRA weights before fusingï¼Œå³ï¼šä¸´æ—¶ç›®å½•ã€‚OpenVLA è®­ç»ƒæ—¶ä¼šå…ˆä¿å­˜ LoRA çš„å°æ–‡ä»¶åˆ°è¿™é‡Œï¼Œç„¶åå†æŠŠå®ƒä»¬åˆå¹¶åˆ°åº•åº§æ¨¡å‹é‡Œå­˜åˆ° run_root_dirã€‚

    # Fine-tuning Parameters
    batch_size: int = 4                                             # Fine-tuning batch sizeï¼Œå³ï¼šå•å¼ æ˜¾å¡ä¸Šçš„ Batch Size
    max_steps: int = 200_000                                        # Max number of fine-tuning stepsï¼Œå³ï¼šæœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆStepï¼‰ï¼Œä¸æ˜¯ Epoch
    save_steps: int = 5000                                          # Interval for checkpoint savingï¼Œå³ï¼šæ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ï¼ˆCheckpointï¼‰
    learning_rate: float = 5e-4                                     # Fine-tuning learning rateï¼Œå³ï¼šå­¦ä¹ ç‡ â€”â€”â€”â€” 5e-4 æ˜¯ LoRA å¾®è°ƒçš„æ ‡å‡†å€¼

    # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚å¦‚æœæ˜¾å­˜åªèƒ½è·‘ batch_size=2ï¼Œä½†ä½ æƒ³è¾¾åˆ° batch_size=16 çš„æ•ˆæœï¼Œå°±æŠŠè¿™ä¸ªè®¾ä¸º 8 (2*8=16)ã€‚å®ƒä¼šç´¯ç§¯ 8 æ¬¡å‰å‘ä¼ æ’­çš„æ¢¯åº¦åï¼Œæ‰æ‰§è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°
    grad_accumulation_steps: int = 1                                # Gradient accumulation stepsï¼Œå³ï¼šæ¢¯åº¦ç´¯ç§¯æ­¥æ•° â€”â€”â€”â€” æ˜¾å­˜ä¸å¤Ÿæ—¶ï¼Œç´¯ç§¯å‡ æ¬¡æ¢¯åº¦å†æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œå˜ç›¸å¢å¤§ Batch Size

    image_aug: bool = True                                          # Whether to train with image augmentationsï¼Œå³ï¼šå›¾åƒå¢å¼º â€”â€”â€”â€” æ˜¯å¦åœ¨è®­ç»ƒæ—¶éšæœºæ”¹å˜äº®åº¦ã€å¯¹æ¯”åº¦ç­‰ï¼Œå¢åŠ æ¨¡å‹æ³›åŒ–æ€§
    shuffle_buffer_size: int = 100_000                              # Dataloader shuffle buffer size (can reduce if OOM)ï¼Œå³ï¼šRLDS æ•°æ®æµçš„éšæœºç¼“å†²åŒºå¤§å°ã€‚è¶Šå¤§éšæœºæ€§è¶Šå¥½ï¼Œä½†è¶Šå å†…å­˜ã€‚

    # å¦‚æœä¸º Trueï¼Œæ¯æ¬¡ä¿å­˜æ—¶ä¼šè¦†ç›–ä¸Šä¸€æ¬¡çš„æ£€æŸ¥ç‚¹ï¼Œåªç•™æœ€æ–°çš„ã€‚èŠ‚çœç¡¬ç›˜ç©ºé—´ã€‚å¦‚æœæ˜¯ Falseï¼Œåˆ™ä¿å­˜æ‰€æœ‰æ¨¡å‹
    save_latest_checkpoint_only: bool = True                        # Whether to save only one checkpoint per run and continually overwrite the latest checkpoint (If False, saves all checkpoints)

    # LoRA Arguments
    use_lora: bool = True                                           # Whether to use LoRA fine-tuningï¼Œå³ï¼šæ˜¯å¦ä½¿ç”¨ LoRAã€‚OpenVLA å¼ºçƒˆå»ºè®® Trueã€‚
    lora_rank: int = 32                                             # Rank of LoRA weight matrixï¼Œå³ï¼šç§©ï¼ˆRankï¼‰â€”â€”â€”â€” LoRA çŸ©é˜µçš„ç»´åº¦ï¼Œè¶Šå¤§å‚æ•°è¶Šå¤šï¼Œæ‹Ÿåˆèƒ½åŠ›è¶Šå¼ºä½†æ˜¾å­˜å ç”¨è¶Šé«˜ã€‚å†³å®šäº†å¾®è°ƒå‚æ•°é‡ã€‚32 æ˜¯å¹³è¡¡ç‚¹ï¼Œå¤ªå°æ‹Ÿåˆä¸å¤Ÿï¼Œå¤ªå¤§æ˜¾å­˜ä¸å¤Ÿã€‚
    lora_dropout: float = 0.0                                       # Dropout applied to LoRA weightsï¼Œå³ï¼šDropout ç‡ â€”â€”â€”â€” é˜²æ­¢è¿‡æ‹Ÿåˆçš„éšæœºå¤±æ´»æ¯”ä¾‹ã€‚0.0 è¡¨ç¤ºä¸ä½¿ç”¨ã€‚

    use_quantization: bool = True                                   # Whether to 4-bit quantize VLA for LoRA fine-tuningï¼Œå³ï¼šé‡åŒ–å¼€å…³ â€”â€”â€”â€” æ˜¯å¦ä½¿ç”¨ 4-bit é‡åŒ–åŠ è½½åº•åº§æ¨¡å‹ï¼ˆå³ QLoRAï¼‰ï¼Œæå¤§èŠ‚çœæ˜¾å­˜
                                                                    #  psï¼š=> CAUTION: Reduces memory but hurts performance â€”â€”> ä¼šé™ä½ä¸€äº›æ€§èƒ½

    # Tracking Parameters
    wandb_project: str = "openvla-debug"                            # Name of W&B project to log to (use default!)
    wandb_entity: str = "stanford-voltron"                          # Name of entity to log under
    run_id_note: Optional[str] = None                               # Extra note for logging, Weights & Biases

    # fmt: on


@draccus.wrap()
def finetune(cfg: FinetuneConfig) -> None:
    print(
        f"Fine-tuning OpenVLA Model `{cfg.vla_path}` on `{cfg.dataset_name}`")

    # [Validate] Ensure GPU Available & Set Device / Distributed Context
    # [æ ¡éªŒ] å¿…é¡»æœ‰ GPU
    assert torch.cuda.is_available(), "Fine-tuning assumes at least one GPU is available!"

    # åˆå§‹åŒ–åˆ†å¸ƒå¼çŠ¶æ€ã€‚PartialState æ˜¯ accelerate åº“æä¾›çš„å·¥å…·ï¼Œå¤„ç†å¤šå¡åˆ†å¸ƒå¼ã€‚
    # å®ƒèƒ½è‡ªåŠ¨è¯†åˆ«ä½ æ˜¯å•å¡è¿è¡Œè¿˜æ˜¯å¤šå¡ DDP è¿è¡Œï¼Œå¹¶è·å–å½“å‰è¿›ç¨‹çš„ ID (local_process_index)ã€‚
    distributed_state = PartialState()

    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„ GPU ID
    # å¼ºåˆ¶è®¾å®šå½“å‰è¿›ç¨‹åªä½¿ç”¨åˆ†é…ç»™å®ƒçš„é‚£å— GPUã€‚æ¯”å¦‚åœ¨ 8 å¡æœºå™¨ä¸Šï¼Œè¿›ç¨‹ 3 å°±åªèƒ½çœ‹åˆ° GPU 3ã€‚
    torch.cuda.set_device(device_id := distributed_state.local_process_index)
    # æ¸…ç©ºæ˜¾å­˜ç¢ç‰‡
    torch.cuda.empty_cache()

    # Configure Unique Experiment ID & Log Directory
    # Notice [æ„å»ºå®éªŒ ID] ç”Ÿæˆä¸€ä¸ªç±»ä¼¼ "openvla-7b+libero_spatial+b16+lr-5e-4..." è¿™æ ·åŒ…å«æ‰€æœ‰å…³é”®å‚æ•°çš„å­—ç¬¦ä¸²
    exp_id = (
        f"{cfg.vla_path.split('/')[-1]}+{cfg.dataset_name}" f"+b{cfg.batch_size * cfg.grad_accumulation_steps}" f"+lr-{cfg.learning_rate}")
    if cfg.use_lora:
        exp_id += f"+lora-r{cfg.lora_rank}+dropout-{cfg.lora_dropout}"
    if cfg.use_quantization:
        exp_id += "+q-4bit"
    if cfg.run_id_note is not None:
        exp_id += f"--{cfg.run_id_note}"
    if cfg.image_aug:
        exp_id += "--image_aug"

    timestamp = datetime.now().strftime("%Y_%m_%d-%H_%M_%S")
    exp_id += f"--{timestamp}"

    # Start =>> Build Directories
    run_dir, adapter_dir = cfg.run_root_dir / exp_id, cfg.adapter_tmp_dir / exp_id
    os.makedirs(run_dir, exist_ok=True)

    # Quantization Config =>> only if LoRA fine-tuning
    quantization_config = None
    # å¦‚æœå¼€å¯é‡åŒ–ï¼Œæ¨¡å‹å°†ä»¥ 4-bit ç²¾åº¦åŠ è½½
    if cfg.use_quantization:
        assert cfg.use_lora, "Quantized training only supported for LoRA fine-tuning!"
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,  # å¼€å¯ 4-bit é‡åŒ–
            # é‡è¦ï¼è®¡ç®—æ—¶å°†ä¸Šé¢ 4-bit åé‡åŒ–å› bfloat16 è¿›è¡Œè®¡ç®—ï¼Œä¿è¯ç²¾åº¦
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_quant_type="nf4"  # ä½¿ç”¨ NormalFloat4 æ•°æ®ç±»å‹ï¼Œé’ˆå¯¹æ­£æ€åˆ†å¸ƒçš„ç¥ç»ç½‘ç»œæƒé‡è®¾è®¡çš„é‡åŒ–æ•°æ®ç±»å‹ï¼Œæ¯”æ ‡å‡†çš„çº¿æ€§é‡åŒ–ç²¾åº¦æ›´é«˜
        )

    # Register OpenVLA model to HF Auto Classes (not needed if the model is on HF Hub)
    # ps æ³¨å†Œ OpenVLA ä¸“å±é…ç½®ç±»ã€‚ç”±äº OpenVLA æ˜¯æ–¯å¦ç¦å›¢é˜Ÿè‡ªå®šä¹‰çš„æ¨¡å‹ç»“æ„ï¼Œæ•…éœ€è¦æ˜¾å¼æ³¨å†Œåˆ°ç³»ç»Ÿçš„è‡ªåŠ¨åŠ è½½å™¨ä¸­
    AutoConfig.register("openvla", OpenVLAConfig)
    # åŠ è½½ä¸€ç³»åˆ—å¤„ç†å™¨ (Processor)ï¼ŒåŒ…å« Tokenizer (å¤„ç†æ–‡æœ¬) å’Œ ImageProcessor (å¤„ç†å›¾åƒ)
    AutoImageProcessor.register(OpenVLAConfig, PrismaticImageProcessor)
    AutoProcessor.register(OpenVLAConfig, PrismaticProcessor)
    AutoModelForVision2Seq.register(OpenVLAConfig, OpenVLAForActionPrediction)

    # Load OpenVLA Processor and Model using HF AutoClasses
    # trust_remote_code=True: å¿…é¡»ä¸º Trueã€‚å…è®¸ä»æ¨¡å‹æ–‡ä»¶å¤¹é‡ŒåŠ è½½ `modeling_openvla.py` è¿™æ ·çš„ python ä»£ç å¹¶æ‰§è¡Œã€‚
    processor = AutoProcessor.from_pretrained(
        cfg.vla_path, trust_remote_code=True)

    # Notice åŠ è½½åŸå§‹å¤§æ¨¡å‹ OpenVLA-7B
    vla = AutoModelForVision2Seq.from_pretrained(
        cfg.vla_path,  # æ¨¡å‹è·¯å¾„
        torch_dtype=torch.bfloat16,  # æ˜¾å¼æŒ‡å®šæƒé‡ç²¾åº¦ä½¿ç”¨ BF16 ç²¾åº¦åŠ è½½ï¼Œé˜²æ­¢é»˜è®¤ FP32 æ’‘çˆ†æ˜¾å­˜
        quantization_config=quantization_config,  # ä¼ å…¥ä¸Šé¢çš„é‡åŒ–é…ç½®
        low_cpu_mem_usage=True,  # ä¼˜åŒ– CPU å†…å­˜åŠ è½½ç­–ç•¥ï¼ˆåˆ†å±‚åŠ è½½ï¼‰ï¼Œé¿å…ä¸€æ¬¡æ€§å ç”¨è¿‡å¤š CPU å†…å­˜
        trust_remote_code=True,  # å…è®¸è¿è¡Œæ¨¡å‹ä»“åº“é‡Œçš„è‡ªå®šä¹‰ Python ä»£ç 
    )

    # Notice ã€å¾®è°ƒçš„æ ¸å¿ƒã€‘LoRA é€‚é…å™¨æŒ‚è½½ (PEFT Setup)
    # Device Placement =>> note that BitsAndBytes automatically handles for quantized training
    if cfg.use_quantization:
        # prepare_model_for_kbit_training: å¯¹é‡åŒ–æ¨¡å‹è¿›è¡Œä¸€ç³»åˆ—å¤„ç†ï¼ˆå¦‚å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ã€è½¬æ¢ LayerNorm ç²¾åº¦ï¼‰ï¼Œä½¿å…¶å¯ä»¥è¢«è®­ç»ƒ
        # è¿™ä¸ªå‡½æ•°çš„ä½œç”¨åŒ…æ‹¬ï¼š
        #   1. å†»ç»“æ‰€æœ‰å‚æ•°ã€‚
        #   2. å°† LayerNorm å±‚å¼ºåˆ¶è½¬å› float32ï¼ˆä¿è¯ç¨³å®šæ€§ï¼‰ã€‚
        #   3. å¼€å¯ gradient checkpointingï¼ˆä»¥è®¡ç®—æ¢æ˜¾å­˜ï¼‰ã€‚
        vla = prepare_model_for_kbit_training(vla)
        # ã€æ–°å¢è¿™è¡Œã€‘å¼ºåˆ¶ä½¿ç”¨ use_reentrant=False æ¥è§£å†³ DDP æŠ¥é”™
        vla.gradient_checkpointing_enable(
            gradient_checkpointing_kwargs={"use_reentrant": False})

    else:
        # è‹¥æœªå¼€å¯é‡åŒ–ï¼Œåˆ™ç›´æ¥ç§»å…¥ GPU
        vla = vla.to(device_id)

    # Notice [LoRA] Wrap Model w/ PEFT `LoraConfig` =>> by default we set `target_modules=all-linear`
    if cfg.use_lora:
        lora_config = LoraConfig(
            r=cfg.lora_rank,  # ç§© (Rank)ï¼Œå†³å®šäº†å¯è®­ç»ƒå‚æ•°é‡çš„å¤§å°ã€‚32 æ˜¯ä¸ªé€‚ä¸­çš„å€¼ (ä¸¤ä¸ªå°çŸ©é˜µçš„å®½åº¦)
            lora_alpha=min(cfg.lora_rank, 16),  # ç¼©æ”¾ç³»æ•°ã€‚é€šå¸¸è®¾ç½®ä¸º r çš„ 1å€æˆ– 2å€
            lora_dropout=cfg.lora_dropout,
            target_modules="all-linear",  # å°† LoRA é€‚é…å™¨æŒ‚è½½åˆ°æ¨¡å‹ä¸­æ‰€æœ‰çš„çº¿æ€§å±‚ï¼ˆåŒ…æ‹¬ Q, K, V, Projection, FFNï¼‰
            init_lora_weights="gaussian",  # ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ– LoRA æƒé‡
        )

        # Notice ã€æ ¸å¿ƒåŠ¨ä½œã€‘æŠŠ LoRA æŒ‚è½½åˆ°æ¨¡å‹ä¸Šå»ï¼Œå³ï¼šå†»ç»“åŸæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œåªå°† LoRA å‚æ•°è®¾ä¸ºå¯è®­ç»ƒï¼ˆrequires_grad=Trueï¼‰
        # å®ƒä¼šåœ¨åŸæ¨¡å‹å¤–éƒ¨å¥—ä¸€å±‚ PeftModelWrapperã€‚æ­¤æ—¶ï¼Œvla.parameters() é‡Œåªæœ‰ LoRA çš„ A, B çŸ©é˜µæ˜¯ requires_grad=True çš„ï¼ŒåŸæœ‰çš„ 7B å‚æ•°å…¨éƒ¨å˜æˆäº† requires_grad=Falseã€‚
        vla = get_peft_model(vla, lora_config)

        # æ‰“å°å¯è®­ç»ƒå‚æ•°é‡ï¼Œç¡®è®¤åªæœ‰æå°‘æ¯”ä¾‹ï¼ˆå¦‚0.2%ï¼‰çš„å‚æ•°å‚ä¸è®­ç»ƒ
        vla.print_trainable_parameters()

    # ps å¤šå¡å¹¶è¡Œè®­ç»ƒï¼šWrap VLA in PyTorch DDP Wrapper for Multi-GPU Training
    # find_unused_parameters=True: å…è®¸æ¨¡å‹ä¸­æœ‰éƒ¨åˆ†å‚æ•°åœ¨å‰å‘ä¼ æ’­ä¸­æœªè¢«ä½¿ç”¨ï¼ˆOpenVLA ä¸­å¸¸è§ï¼‰
    vla = DDP(
        vla,
        device_ids=[device_id],
        find_unused_parameters=True,
        gradient_as_bucket_view=True  # å‡å°‘å†…å­˜æ‹·è´ï¼Œä¼˜åŒ–æ˜¾å­˜
    )

    # Create Optimizer =>> note that we default to a simple constant learning rate!
    # åˆ›å»ºä¼˜åŒ–å™¨ã€‚è¿‡æ»¤å‡º requires_grad=True çš„å‚æ•°ï¼Œåªä¼ å…¥ trainable_paramsï¼ˆä¹Ÿå°±æ˜¯åªæœ‰ LoRA å‚æ•°ï¼‰
    trainable_params = [param for param in vla.parameters()
                        if param.requires_grad]
    optimizer = AdamW(trainable_params, lr=cfg.learning_rate)

    # Create Action Tokenizerï¼šå°†è¿ç»­çš„æœºæ¢°è‡‚åŠ¨ä½œæ•°å€¼ï¼ˆå¦‚ x=0.25ï¼‰ç¦»æ•£åŒ–ä¸º Token IDï¼ˆå¦‚ 512ï¼‰
    # ps è¿™æ˜¯ OpenVLA/RT-2 æ¶æ„çš„æ ¸å¿ƒï¼ŒæŠŠåŠ¨ä½œé¢„æµ‹å˜æˆäº†â€œå®Œå½¢å¡«ç©ºâ€çš„åˆ†ç±»é—®é¢˜
    action_tokenizer = ActionTokenizer(processor.tokenizer)

    # Load Fine-tuning Dataset =>> note that we use an RLDS-formatted dataset following Open X-Embodiment by default.
    #   =>> If you want to use a non-RLDS dataset (e.g., a standard PyTorch Dataset) see the following commented block.
    #   =>> Note that our training code does not loop over epochs because the RLDS loader does this implicitly; if using
    #       your own Dataset, make sure to add the appropriate logic to the training loop!
    #
    # ---
    # from prismatic.vla.datasets import DummyDataset
    #
    # vla_dataset = DummyDataset(
    #     action_tokenizer,
    #     processor.tokenizer,
    #     image_transform=processor.image_processor.apply_transform,
    #     prompt_builder_fn=PurePromptBuilder if "v01" not in cfg.vla_path else VicunaV15ChatPromptBuilder,
    # )
    # ---

    # RLDSBatchTransform: è¿™æ˜¯ä¸€ä¸ªå›è°ƒå‡½æ•°ç±»
    # å®šä¹‰æ•°æ®é¢„å¤„ç†é€»è¾‘ï¼šå›¾ç‰‡ç¼©æ”¾ã€æŒ‡ä»¤æ·»åŠ  Prompt æ¨¡æ¿ç­‰
    batch_transform = RLDSBatchTransform(
        action_tokenizer,
        processor.tokenizer,
        # æ„å»ºæç¤ºè¯æ¨¡æ¿ï¼ˆæ¯”å¦‚ "USER: What to do? ASSISTANT: <action>"ï¼‰
        image_transform=processor.image_processor.apply_transform,
        # æ„å»ºæç¤ºè¯æ¨¡æ¿ï¼ˆæ¯”å¦‚ "USER: What to do? ASSISTANT: <action>"ï¼‰
        prompt_builder_fn=PurePromptBuilder if "v01" not in cfg.vla_path else VicunaV15ChatPromptBuilder,
    )

    # RLDSDataset: å®é™…ä¸Šæ˜¯è°ƒç”¨çš„ dlimp åº“å»ä¸“é—¨è¯»å– TFRecord æ ¼å¼çš„ RLDS æ•°æ®
    vla_dataset = RLDSDataset(
        cfg.data_root_dir,
        cfg.dataset_name,
        batch_transform,
        resize_resolution=tuple(vla.module.config.image_sizes),  # å†æ¬¡ç¡®è®¤å›¾ç‰‡å°ºå¯¸
        shuffle_buffer_size=cfg.shuffle_buffer_size,  # å†³å®šæ•°æ®æ‰“ä¹±çš„ç¨‹åº¦
        image_aug=cfg.image_aug,
    )

    # Notice [Important] Save Dataset Statistics =>> used to de-normalize actions for inference!
    if distributed_state.is_main_process:
        save_dataset_statistics(vla_dataset.dataset_statistics, run_dir)

    # Create Collator and DataLoaderï¼šæ•´ç†å™¨
    # å› ä¸ºæ¯ä¸ª batch é‡Œçš„æŒ‡ä»¤é•¿åº¦å¯èƒ½ä¸ä¸€æ ·ï¼ˆæœ‰çš„æŒ‡ä»¤é•¿æœ‰çš„çŸ­ï¼‰ï¼ŒCollator è´Ÿè´£ç”¨ pad_token å°†å®ƒä»¬å¡«å……å¯¹é½åˆ°å½“å‰ batch çš„æœ€å¤§é•¿åº¦ï¼Œè¿™æ ·æ‰èƒ½å †å æˆ Tensorã€‚
    collator = PaddedCollatorForActionPrediction(
        processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side="right")

    # é€šå¸¸æƒ…å†µ PyTorch è®­ç»ƒæˆ‘ä»¬ä¼šè®¾ä¸º 4 æˆ– 8ã€‚
    # ä½†æ­¤å¤„å¿…é¡»è®¾ä¸º 0ï¼å› ä¸º RLDS åº•å±‚ä¾èµ– TensorFlow çš„ data loaderï¼Œå®ƒè‡ªå·±å†…éƒ¨å·²ç»æœ‰å¤šçº¿ç¨‹å¹¶è¡Œäº†ã€‚
    # å¦‚æœ PyTorch å†å¼€å¤šè¿›ç¨‹å» fork TensorFlow çš„è¿›ç¨‹ï¼Œä¼šå¯¼è‡´æ­»é”æˆ–æ˜¾å­˜çˆ†ç‚¸ï¼
    dataloader = DataLoader(
        vla_dataset,
        batch_size=cfg.batch_size,
        sampler=None,
        collate_fn=collator,  # è´Ÿè´£å°†ä¸åŒé•¿åº¦çš„æ•°æ® Padding åˆ°åŒä¸€é•¿åº¦
        num_workers=0,  # ps Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!
    )

    # Initialize Logging =>> W&B
    if distributed_state.is_main_process:
        wandb.init(entity=cfg.wandb_entity,
                   project=cfg.wandb_project, name=f"ft+{exp_id}")

    # Deque to store recent train metrics (used for computing smoothened metrics for gradient accumulation)
    # ç”¨äºå¹³æ»‘æ—¥å¿—æ›²çº¿çš„é˜Ÿåˆ—
    recent_losses = deque(maxlen=cfg.grad_accumulation_steps)
    recent_action_accuracies = deque(maxlen=cfg.grad_accumulation_steps)
    recent_l1_losses = deque(maxlen=cfg.grad_accumulation_steps)

    # Note Train!
    with tqdm.tqdm(total=cfg.max_steps, leave=False) as progress:

        # è¿›å…¥è®­ç»ƒæ¨¡å¼
        vla.train()

        optimizer.zero_grad()

        # å¼€å§‹å¾ªç¯ Batch
        for batch_idx, batch in enumerate(dataloader):
            # 1. å¼€å¯æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡ (BF16)ï¼Œè‡ªåŠ¨å°†è®¡ç®—è½¬ä¸º bfloat16ï¼ŒåŠ å¿«é€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜
            with torch.autocast("cuda", dtype=torch.bfloat16):
                # è¯¦è§£ï¼šOpenVLA æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªâ€œå› æœè¯­è¨€æ¨¡å‹â€ï¼ˆCausal LMï¼Œå°±åƒ GPT ä¸€æ ·ï¼‰ã€‚å½“è¿è¡Œæ¨¡å‹æ—¶ï¼Œå®ƒè¿”å›çš„ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ•°å­—ï¼Œè€Œæ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªå­—æ®µçš„åŒ…è£¹ï¼š
                #   loss: æŸå¤±å€¼ï¼ˆè®­ç»ƒæ—¶ç”¨æ¥åå‘ä¼ æ’­ï¼‰ã€‚
                #   logits: æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ª Token çš„æ¦‚ç‡åˆ†å¸ƒã€‚
                #   past_key_values: KV Cacheï¼ˆç”¨äºåŠ é€Ÿæ¨ç†çš„ç¼“å­˜ï¼Œè™½ç„¶å¾®è°ƒæ—¶é€šå¸¸ç”¨ä¸åˆ°ï¼Œä½†ç»“æ„é‡Œå¿…é¡»æœ‰ï¼‰ã€‚
                # ä½œç”¨ï¼šä¸ºäº† ç±»å‹æç¤º (Type Hinting)ã€‚å‘Šè¯‰é˜…è¯»ä»£ç çš„äººå’Œ IDE å·¥å…·ï¼šâ€œæ­¤ vla() å‡½æ•°è·‘å®Œåï¼Œä¼šè¾“å‡ºä¸€ä¸ªæ ‡å‡†çš„ LM è¾“å‡ºå¯¹è±¡â€ï¼Œæ–¹ä¾¿å¼€å‘äººå‘˜çŸ¥é“æ€ä¹ˆå»å–é‡Œé¢çš„ .loss æˆ– .logits
                output: CausalLMOutputWithPast = vla(
                    input_ids=batch["input_ids"].to(device_id),  # æ–‡æœ¬æŒ‡ä»¤
                    attention_mask=batch["attention_mask"].to(device_id),
                    pixel_values=batch["pixel_values"].to(
                        torch.bfloat16).to(device_id),  # å›¾åƒæ•°æ® tensor
                    labels=batch["labels"],  # åŠ¨ä½œçš„çœŸå€¼ï¼ˆGround Truthï¼‰ï¼Œç”¨äºè®¡ç®— Loss
                )

                # æ¨¡å‹å†…éƒ¨è‡ªåŠ¨è®¡ç®— CrossEntropyLoss
                loss = output.loss

            # æ¢¯åº¦ç´¯ç§¯å½’ä¸€åŒ–ï¼šå› ä¸º loss æ˜¯ç´¯ç§¯å¤šæ¬¡æ‰ stepï¼Œæ‰€ä»¥è¿™é‡Œè¦é™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼Œä¿æŒæ¢¯åº¦å°ºåº¦ä¸€è‡´
            normalized_loss = loss / cfg.grad_accumulation_steps

            # åå‘ä¼ æ’­ (Backward)ï¼šè®¡ç®—æ¢¯åº¦
            normalized_loss.backward()

            ################################ æ—¥å¿—è®°å½•ç›¸å…³ï¼Œå¯å¿½ç•¥ğŸ‘‡ ################################
            # Compute Accuracy and L1 Loss for Logging
            action_logits = output.logits[:,
                                          vla.module.vision_backbone.featurizer.patch_embed.num_patches: -1]
            action_preds = action_logits.argmax(dim=2)
            action_gt = batch["labels"][:, 1:].to(action_preds.device)
            mask = action_gt > action_tokenizer.action_token_begin_idx

            # Compute Accuracy
            correct_preds = (action_preds == action_gt) & mask
            action_accuracy = correct_preds.sum().float() / mask.sum().float()

            # Compute L1 Loss on Predicted (Continuous) Actions
            continuous_actions_pred = torch.tensor(
                action_tokenizer.decode_token_ids_to_actions(action_preds[mask].cpu().numpy()))
            continuous_actions_gt = torch.tensor(
                action_tokenizer.decode_token_ids_to_actions(action_gt[mask].cpu().numpy()))
            action_l1_loss = torch.nn.functional.l1_loss(
                continuous_actions_pred, continuous_actions_gt)

            # Store recent train metrics
            recent_losses.append(loss.item())
            recent_action_accuracies.append(action_accuracy.item())
            recent_l1_losses.append(action_l1_loss.item())

            # Compute gradient step index
            gradient_step_idx = batch_idx // cfg.grad_accumulation_steps

            # Compute smoothened train metrics
            #   =>> Equal to current step metrics when not using gradient accumulation
            #   =>> Otherwise, equal to the average of metrics observed over micro-batches used for gradient accumulation
            smoothened_loss = sum(recent_losses) / len(recent_losses)
            smoothened_action_accuracy = sum(
                recent_action_accuracies) / len(recent_action_accuracies)
            smoothened_l1_loss = sum(recent_l1_losses) / len(recent_l1_losses)

            # Push Metrics to W&B (every 10 gradient steps)
            if distributed_state.is_main_process and gradient_step_idx % 10 == 0:
                wandb.log(
                    {
                        "train_loss": smoothened_loss,
                        "action_accuracy": smoothened_action_accuracy,
                        "l1_loss": smoothened_l1_loss,
                    },
                    step=gradient_step_idx,
                )
            ####################################################################################

            # Notice Optimizer Stepï¼šä¼˜åŒ–å™¨æ›´æ–°
            # åªæœ‰å½“ç´¯ç§¯äº†è¶³å¤Ÿçš„æ­¥æ•°åï¼Œæ‰çœŸæ­£æ›´æ–°ä¸€æ¬¡å‚æ•°ã€‚
            if (batch_idx + 1) % cfg.grad_accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦ï¼Œé˜²æ­¢ç´¯ç§¯åˆ°ä¸‹ä¸€è½®
                progress.update()  # è¿›åº¦æ¡ +1

            # Notice ã€Save Model Checkpointã€‘=>> by default, only keeps the latest checkpoint, continually overwriting it!
            # ps è¿™æ˜¯ OpenVLA å¤ç°ä¸­æœ€ç‹¬ç‰¹çš„ä¸€æ­¥ï¼
            if gradient_step_idx > 0 and gradient_step_idx % cfg.save_steps == 0:
                if distributed_state.is_main_process:
                    print(
                        f"Saving Model Checkpoint for Step {gradient_step_idx}")

                    # If LoRA, we first save adapter weights, then merge into full model; otherwise, default save!
                    save_dir = adapter_dir if cfg.use_lora else run_dir

                    # Save Processor & Weights
                    # step1. ä¿å­˜ Processor (åŒ…å« Tokenizer é…ç½®ç­‰)ï¼Œæ­¤æ—¶åªä¿å­˜äº†å‡ ç™¾ MB çš„ adapter æ–‡ä»¶å¤¹
                    processor.save_pretrained(run_dir)
                    # step2. ä¿å­˜ Adapter (LoRA æƒé‡) åˆ°ä¸´æ—¶ç›®å½•
                    vla.module.save_pretrained(save_dir)

                # åˆ†å¸ƒå¼åŒæ­¥å±éšœï¼šç¡®ä¿ä¸»è¿›ç¨‹ä¿å­˜å®Œï¼Œå…¶ä»–è¿›ç¨‹å†ç»§ç»­ï¼Œé˜²æ­¢æ–‡ä»¶è¯»å†™å†²çªã€‚
                dist.barrier()

                # Merge LoRA weights into model backbone for faster inference
                #   =>> Note that merging is slow and can be done post-hoc to speed up training
                # step3. [åˆå¹¶æƒé‡] Merge Logicï¼ï¼ï¼
                # ä¸ºäº†æ¨ç†æ–¹ä¾¿ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›æ¯æ¬¡æ¨ç†éƒ½åˆ†åˆ«åŠ è½½ Base æ¨¡å‹å’Œ Adapterã€‚æ‰€ä»¥è¿™é‡Œåšäº†ä¸€ä¸ªâ€œèåˆâ€æ“ä½œã€‚
                if cfg.use_lora:
                    # é‡æ–°åŠ è½½ä¸€ä¸ªå¹²å‡€çš„ã€æœªå†»ç»“çš„åº•åº§æ¨¡å‹ (base_vla)
                    base_vla = AutoModelForVision2Seq.from_pretrained(
                        cfg.vla_path,
                        torch_dtype=torch.bfloat16,
                        low_cpu_mem_usage=True,
                        trust_remote_code=True
                    )

                    # åŠ è½½åˆšæ‰ä¿å­˜çš„ LoRA æƒé‡
                    merged_vla = PeftModel.from_pretrained(
                        base_vla, adapter_dir)

                    # æ ¸å¿ƒå‡½æ•° merge_and_unload(): å°† LoRA çš„çŸ©é˜µä¹˜ç§¯åŠ å›åˆ°åŸæ¨¡å‹çš„æƒé‡çŸ©é˜µä¸­
                    # æ•°å­¦åŸç†ï¼šW_new = W_base + (A * B * scale)
                    # æ‰§è¡Œå®Œåï¼Œmerged_vla å°±å˜æˆäº†ä¸€ä¸ªæ™®é€šçš„æ¨¡å‹ï¼Œæ²¡æœ‰ LoRA å±‚äº†ï¼Œä½†æƒé‡å·²ç»åŒ…å«äº†å¾®è°ƒçš„ä¿¡æ¯ã€‚
                    # ç»“æœæ˜¯ä¸€ä¸ªç»“æ„ä¸åŸæ¨¡å‹å®Œå…¨ä¸€è‡´ï¼Œä½†å‚æ•°å·²æ›´æ–°çš„æ ‡å‡†æ¨¡å‹ã€‚
                    merged_vla = merged_vla.merge_and_unload()

                    if distributed_state.is_main_process:
                        # step4. ä¿å­˜æœ€ç»ˆçš„èåˆæ¨¡å‹
                        if cfg.save_latest_checkpoint_only:
                            # Overwrite latest checkpointï¼šä¿å­˜æœ€ç»ˆçš„å…¨é‡æ¨¡å‹ï¼Œè¿™æ ·æ¨ç†æ—¶å°±ä¸éœ€è¦åŠ è½½ä¸¤ä¸ªæ–‡ä»¶ï¼Œç›´æ¥åŠ è½½è¿™ä¸€ä¸ªå¤§æ¨¡å‹å³å¯
                            merged_vla.save_pretrained(run_dir)

                            print(
                                f"Saved Model Checkpoint for Step {gradient_step_idx} at: {run_dir}")
                        else:
                            # Prepare to save checkpoint in new directory
                            checkpoint_dir = Path(
                                str(run_dir) + f"--{gradient_step_idx}_chkpt")
                            os.makedirs(checkpoint_dir, exist_ok=True)

                            # Save dataset statistics to new directory
                            save_dataset_statistics(
                                vla_dataset.dataset_statistics, checkpoint_dir)

                            # Save processor and model weights to new directory
                            processor.save_pretrained(checkpoint_dir)
                            merged_vla.save_pretrained(checkpoint_dir)

                            print(
                                f"Saved Model Checkpoint for Step {gradient_step_idx} at: {checkpoint_dir}")

                # Block on Main Process Checkpointing
                dist.barrier()

            # Stop training when max_steps is reached
            if gradient_step_idx == cfg.max_steps:
                print(
                    f"Max step {cfg.max_steps} reached! Stopping training...")
                break


if __name__ == "__main__":
    finetune()
